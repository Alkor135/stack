## Решающие деревья и ансамбли


```python
import matplotlib.pyplot as plt
import scipy.stats as sps
import numpy as np
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
import pydotplus 
from IPython.display import Image
from sklearn import tree

from sklearn.datasets import load_iris, load_digits, load_breast_cancer, load_boston
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, train_test_split
from sklearn.base import BaseEstimator
import matplotlib.pyplot as plt
import collections

from matplotlib.colors import ListedColormap
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_distances

import IPython.display

from sklearn import tree
import sklearn
import xgboost as xgb
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Lasso, Ridge
```

# 1. Визуализация


```python
data = load_iris()
X = data.data[:, [False, True, True, False]]
y = data.target

fn = np.array(data["feature_names"])[[False, True, True, False]]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)
```


```python
def magic_plot(clf):
    cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    plt.figure(figsize=(7, 7))
    k = 1

    h = 400
    xx, yy = np.meshgrid(
        np.linspace(X[:, 0].min(), X[:, 0].max(), h),
        np.linspace(X[:, 1].min(), X[:, 1].max(), h)
    )

    X_grid = np.c_[xx.ravel(), yy.ravel()]

    clf.fit(X_train, y_train)
    Z = clf.predict(X_grid)
    acc = accuracy_score(y_test, clf.predict(X_test))

    plt.title("k = {}, accuracy = {}".format(k, round(acc, 3)))
    zz = np.array(Z).reshape(xx.shape)
    plt.pcolormesh(xx, yy, zz, cmap=cmap)
    plt.xlabel(data["feature_names"][1])
    plt.ylabel(data["feature_names"][2])
    for i in range(3):
        plt.scatter(X[y == i, 0], X[y == i, 1], color=["red", "green", "blue"][i], label=data["target_names"][i])
    plt.legend()

    plt.show()
    return clf
```


```python
magic_plot(KNeighborsClassifier(n_neighbors = 1))
```


    
![png](%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_files/%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_5_0.png)
    





    KNeighborsClassifier(n_neighbors=1)




```python
clf = magic_plot(DecisionTreeClassifier())
```


    
![png](%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_files/%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_6_0.png)
    



```python
def draw_decision_tree(clf, column_names=None):
    dot_data = tree.export_graphviz(clf, out_file="small_tree.out",
                                    feature_names=column_names) 
    
    graph = pydotplus.graphviz.graph_from_dot_file("small_tree.out")  
    a = IPython.display.Image(graph.create_png())
    IPython.display.display(a)
```


```python
draw_decision_tree(clf, fn)
```


    
![png](%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_files/%D0%A0%D0%B5%D1%88%D0%B0%D1%8E%D1%89%D0%B8%D0%B5%20%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D1%8F%20%D0%B8%20%D0%B0%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_8_0.png)
    



```python
# clf = <... постройте такой же график, но ограничьте высоту дерева max_depth = 2 ...>
```


```python
# draw_decision_tree(clf, fn)
```

А упало ли значимо качество?

____________________
# 2. Простой градиентный бустинг


```python
data = sklearn.datasets.load_boston()
print(data.DESCR)
```

    .. _boston_dataset:
    
    Boston house prices dataset
    ---------------------------
    
    **Data Set Characteristics:**  
    
        :Number of Instances: 506 
    
        :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.
    
        :Attribute Information (in order):
            - CRIM     per capita crime rate by town
            - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
            - INDUS    proportion of non-retail business acres per town
            - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
            - NOX      nitric oxides concentration (parts per 10 million)
            - RM       average number of rooms per dwelling
            - AGE      proportion of owner-occupied units built prior to 1940
            - DIS      weighted distances to five Boston employment centres
            - RAD      index of accessibility to radial highways
            - TAX      full-value property-tax rate per $10,000
            - PTRATIO  pupil-teacher ratio by town
            - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town
            - LSTAT    % lower status of the population
            - MEDV     Median value of owner-occupied homes in $1000's
    
        :Missing Attribute Values: None
    
        :Creator: Harrison, D. and Rubinfeld, D.L.
    
    This is a copy of UCI ML housing dataset.
    https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
    
    
    This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.
    
    The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
    prices and the demand for clean air', J. Environ. Economics & Management,
    vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics
    ...', Wiley, 1980.   N.B. Various transformations are used in the table on
    pages 244-261 of the latter.
    
    The Boston house-price data has been used in many machine learning papers that address regression
    problems.   
         
    .. topic:: References
    
       - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
       - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
    
    

    /usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.
    
        The Boston housing prices dataset has an ethical problem. You can refer to
        the documentation of this function for further details.
    
        The scikit-learn maintainers therefore strongly discourage the use of this
        dataset unless the purpose of the code is to study and educate about
        ethical issues in data science and machine learning.
    
        In this special case, you can fetch the dataset from the original
        source::
    
            import pandas as pd
            import numpy as np
    
    
            data_url = "http://lib.stat.cmu.edu/datasets/boston"
            raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
            data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
            target = raw_df.values[1::2, 2]
    
        Alternative datasets include the California housing dataset (i.e.
        :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing
        dataset. You can load the datasets as follows::
    
            from sklearn.datasets import fetch_california_housing
            housing = fetch_california_housing()
    
        for the California housing dataset and::
    
            from sklearn.datasets import fetch_openml
            housing = fetch_openml(name="house_prices", as_frame=True)
    
        for the Ames housing dataset.
        
      warnings.warn(msg, category=FutureWarning)
    


```python
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (379, 13) (127, 13) (379,) (127,)
    


```python
class GradientBoosting:
    def __init__(self, coeff):
        self.get_coeff = coeff
        
    def antigrad(y, pred):
        return <... вычислите антиградиент для квадратичной функции потерь ...>

    def fit(self, X, y):
        estimator = DecisionTreeRegressor(max_depth=5, random_state=42)
        estimator.fit(X, y)

        self.base_algorithms_list = [estimator] 
        self.coefficients_list = [self.get_coeff(0)]

        for i in range(1, 50):
            estimator = <... создайте экземпляр DecisionTreeRegressor так же, как и выше ...>
            pred = self.predict(X)
            s = GradientBoosting.antigrad(y, pred)
            
            # ... обучите estimator на X, s ...
            
            self.base_algorithms_list.append(estimator)
            self.coefficients_list.append(self.get_coeff(i))
            
    def predict(self, X):
        return self.predict_pref(X, len(self.base_algorithms_list))
    
    def predict_pref(self, X, i):
        return [
                sum([ <... допишите недостающее ...>
                     for algo, coeff in zip(self.base_algorithms_list[:i], self.coefficients_list[:i])])
                        for x in X]
    
    def get_mse_diagram(self, X, y):
        grid = np.arange(len(self.base_algorithms_list)) + 1
        MSE = [mean_squared_error(self.predict_pref(X, i), y)
               for i in grid]
        return grid, MSE
```


      File "<ipython-input-20-7ad23ea867e7>", line 6
        return <... вычислите антиградиент для квадратичной функции потерь ...>
               ^
    SyntaxError: invalid syntax
    



```python
estimator_1 = GradientBoosting(coeff = lambda i : 0.1)
estimator_1.fit(X_train, y_train)
grid, MSE_1 = estimator_1.get_mse_diagram(X_test, y_test)

estimator_2 = GradientBoosting(coeff = lambda i : 0.1 / (1.0 + i))
estimator_2.fit(X_train, y_train)
grid, MSE_2 = estimator_2.get_mse_diagram(X_test, y_test)
```


```python
plt.figure(figsize=(10, 5))
plt.plot(grid, MSE_1, label="MSE_1")
plt.plot(grid, MSE_2, label="MSE_2")
plt.ylabel("MSE")
plt.xlabel("base_algorithms")
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(grid, MSE_1, label="MSE_1")
plt.plot(grid, MSE_2, label="MSE_2")
plt.ylabel("MSE")
plt.xlabel("base_algorithms")
plt.legend()
plt.grid()
plt.show()
```


```python
print(mean_squared_error(estimator_1.predict(X_test), y_test)**0.5)
print(mean_squared_error(estimator_2.predict(X_test), y_test)**0.5)
```

Попытайтесь выставить коэффициент 0.9, 0.9 / (1 + i) и посмотрите, что будет. Попробуйте подобрать хорошее правило выбора коэффициентов.

### sklearn / XGBoost


```python
result_xgb = []
result_sklearn = []
ns_estimators = np.arange(2, 301, 5) + 1
for n_estimators in ns_estimators:
    estimator = xgb.XGBRegressor(n_estimators=n_estimators)
    estimator.fit(X_train, y_train)
    result_xgb.append(mean_squared_error(estimator.predict(X_test), y_test))
    
    estimator = GradientBoostingRegressor(n_estimators=n_estimators)
    estimator.fit(X_train, y_train)
    result_sklearn.append(mean_squared_error(estimator.predict(X_test), y_test))
```


```python
plt.figure(figsize=(10, 5))
plt.plot(ns_estimators, result_xgb, label="xgb")
plt.plot(ns_estimators, result_sklearn, label="sklearn")
plt.grid()
plt.legend()
plt.ylim((0, 50))
plt.show()
```


```python
print(np.array(result_xgb).min() ** 0.5)
print(np.array(result_sklearn).min() ** 0.5)
```

### Случайный лес


```python
result_rf = []
ns_estimators = np.arange(2, 301, 5) + 1
for n_estimators in ns_estimators:
    estimator = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1)
    estimator.fit(X_train, y_train)
    result_rf.append(mean_squared_error(estimator.predict(X_test), y_test))
```


```python
plt.figure(figsize=(10, 5))
plt.plot(ns_estimators, result_xgb, label="xgb")
plt.plot(ns_estimators, result_sklearn, label="sklearn")
plt.plot(ns_estimators, result_rf, label="rf")
plt.grid()
plt.legend()
plt.ylim((0, 50))
plt.show()
```


```python
estimator = RandomForestRegressor(n_estimators=5000, n_jobs=-1)
estimator.fit(X_train, y_train)
mean_squared_error(estimator.predict(X_test), y_test) ** 0.5
```

Сравните качество с какой-нибудь известной вам моделью, кроме деревьев и ансамблей и решающим деревом


```python

```


```python

```

# 3. Решающее дерево


```python
def getp(y):
    uniq, counts = np.unique(y, return_counts=True)
    p = counts /  float(len(y))
    return p  # доли обьектов

def entropy(y):  
    p = getp(y)
    return < ... >

def gini(y):
    p = getp(y)
    return < ... >

def variance(y):
    return  < ... >

def mad_median(y):
    return  < ... >

def most_often(y):
    uniq, counts = np.unique(y, return_counts=True)
    return uniq[counts.argmax()]
```


```python
class DecisionTree(BaseEstimator):
    def __init__(self, max_depth=np.inf, min_samples_split=2,
                 criterion='gini', debug=False, classes=None):  # max_split_step
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.criterion = criterion
        self.debug = debug
        self._classes = classes

        self._criterion_map = {
            "gini": gini,
            "entropy": entropy,
            "variance": variance,
            "mad_median": mad_median
        }
        self._criterion_func = self._criterion_map[criterion]
        # self.max_split_step = max_split_step

    def get_params(self, deep=False):
        return {
            "max_depth": self.max_depth,
            "min_samples_split": self.min_samples_split,
            "criterion": self.criterion,
            "debug": self.debug,
        }

    def fit(self, X, y, categorical=None):
        assert len(X.shape) == 2
        assert len(y.shape) == 1
        assert X.shape[0] == len(y)
        assert len(y) > 0
        assert (categorical is None) or (len(categorical) == X.shape[1])

        return self._raw_fit(X, y, categorical)

    def _raw_fit(self, X, y, categorical=None):
        if self.debug:
            print("Fit")
            print(X)
            print(y)

        self._leaf = False

        self._features_count = X.shape[1]
        best_scores = None
        F = self._criterion_func

        uniq_targets = np.unique(y)
        if ((self.max_depth == 0) or (len(uniq_targets) == 1)
                or (X.shape[0] < self.min_samples_split)):
            self._leaf = True
            # Выполнен критерий остановки, нужно найти прогноз в этом листе исходя из значений целевой переменной y
            
            if self.criterion == "variance":
                self._prediction = < ... минимум для MSE ... >
            elif self.criterion == "mad_median":
                self._prediction = < ... минимум для MAE ...  >
            else:
                self._prediction = < ... ответ для классификации ... >
                
            return

        F_y = F(y)
        for f_i in range(self._features_count):
            uniq_values = np.unique(X[:, f_i])
            if (not categorical is None) and (categorical[f_i]):
                borders = uniq_values
            else:
                borders = (uniq_values[:-1] + uniq_values[1:]) / 2.

            for value in borders:  # value - порог, который мы перебираем из borders
                y_l = y[X[:, f_i] < value]
                y_r = < ... вторая половина разбиения ... >
                if len(y_l) > 0 and len(y_r) > 0:
                    k_l = < ... коэффициенты для уменьшения среднего индекса однородности ... >
                    k_r =  < ... коэффициенты для уменьшения среднего индекса однородности ... >
                    F_score = F_y - k_l * F(y_l) - k_r * F(y_r)

                    score = (F_score, f_i, value)
                    if (best_scores is None) or (best_scores < score):
                        best_scores = score

        self._f_i = best_scores[1]
        self._value = best_scores[2]

        params = self.get_params()
        params["max_depth"] = params["max_depth"] - 1
        params["classes"] = self._classes
        
        self._lson = DecisionTree(**params)  # << вызов построения для сыновей
        self._rson = DecisionTree(**params)

        l_indexes = X[:, self._f_i] < self._value
        r_indexes = True ^ l_indexes
        self._lson.fit(X[l_indexes, :], y[l_indexes])
        self._rson.fit(X[r_indexes, :], y[r_indexes])

    def _raw_predict(self, X):
        if X.shape[0] == 0:
            return np.empty(shape=(0))
        
        if self._leaf:
            return np.array([self._prediction] * X.shape[0])

        l_indexes = X[:, self._f_i] < self._value
        r_indexes = True ^ l_indexes
        
        l_predictions = < ... рекурсивно вызываемся на левом сыне и предсказываем значения X[l_indexes] .. >
        r_predictions = < ... то же самое для правого сына .. >

        dtype = l_predictions.dtype if len(l_predictions) > 0 else r_predictions.dtype
        prediction = None

        prediction = np.empty(shape=X.shape[0], dtype=dtype)
        prediction[l_indexes] = l_predictions  # собрали предсказания
        prediction[r_indexes] = r_predictions

        return prediction

    def predict(self, X):
        assert len(X.shape) == 2
        assert X.shape[1] == self._features_count
        return self._raw_predict(X)
```

_____________________
Оценим качество классификации


```python
digits = sklearn.datasets.load_digits()
print(digits.DESCR)
```


```python
X = digits.data
y = digits.target
X.shape, y.shape
```


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```


```python
dt = DecisionTree(max_depth=7, criterion="gini")
print(DecisionTree)
dt.fit(X_train, y_train)
accuracy_score(dt.predict(X_test), y_test)
```


```python
dt = DecisionTree(max_depth=7, criterion="entropy")
print(DecisionTree)
dt.fit(X_train, y_train)
accuracy_score(dt.predict(X_test), y_test)
```


```python
dt = DecisionTree(max_depth=7, criterion="entropy")
print(DecisionTree)
dt.fit(X_train, y_train)
accuracy_score(dt.predict(X_test), y_test)
```


```python
dt = DecisionTreeClassifier(max_depth=7, criterion="entropy")
print(DecisionTreeClassifier)
dt.fit(X_train, y_train)
accuracy_score(dt.predict(X_test), y_test)
```


```python
%%time
param_grid = {"max_depth":range(3, 11), "criterion":["gini", "entropy"]}
model = DecisionTree()
gscv = GridSearchCV(estimator=model, param_grid=param_grid, scoring="accuracy", cv=5, n_jobs=-1)
gscv.fit(X, y)
gscv
```


```python
gscv.best_score_, gscv.best_params_
```


```python
import warnings
warnings.filterwarnings("ignore")
```


```python
plt.figure(figsize=(12, 6))
for bounds in [(0, 11 -3), (11-3, 2 * (11-3))]:
    grid = [gscv.grid_scores_[i][0]["max_depth"] for i in range(*bounds)]
    accuracy = [gscv.grid_scores_[i][1] for i in range(*bounds)]
    criterion = gscv.grid_scores_[bounds[0]][0]["criterion"]
    plt.plot(grid, accuracy, label=criterion)
plt.scatter([gscv.best_params_["max_depth"]], [gscv.best_score_], color="red", label="best result")
plt.grid()
plt.legend()
plt.show()
```

________________
Оценим качество регрессии


```python
boston = sklearn.datasets.load_boston()
X = boston.data
y = boston.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```


```python
from sklearn.metrics import mean_squared_error
for criterion in ["variance", "mad_median"]:
    dt = DecisionTree(criterion=criterion, max_depth=7)
    dt.fit(X_train, y_train)
    mse = mean_squared_error(dt.predict(X_test), y_test)
    print("My DT. Criterion = {}, score = {}".format(criterion, mse))
```


```python
from sklearn.tree import DecisionTreeRegressor
for  criterion in ["mse", "mae"]:
    dt = DecisionTreeRegressor(criterion=criterion, max_depth=7)
    dt.fit(X_train, y_train)
    mse = mean_squared_error(dt.predict(X_test), y_test)
    print("Sklearn DT : Criterion = {}, score = {}".format(criterion, mse))
```


```python
%%time
param_grid = {"max_depth":range(2, 19), "criterion":["variance", "mad_median"]}
model = DecisionTree()
print(type(model))
gscv = GridSearchCV(estimator=model, param_grid=param_grid, scoring="neg_mean_squared_error", cv=5, n_jobs=-1)
gscv.fit(X, y)
gscv
```


```python
plt.figure(figsize=(12, 6))
l = int(len(gscv.grid_scores_) / 2)

for bounds in [(0, l), (l, 2 * l)]:
    grid = [gscv.grid_scores_[i][0]["max_depth"] for i in range(*bounds)]
    accuracy = [gscv.grid_scores_[i][1] for i in range(*bounds)]
    criterion = gscv.grid_scores_[bounds[0]][0]["criterion"]
    plt.plot(grid, accuracy, label=criterion)
plt.scatter([gscv.best_params_["max_depth"]], [gscv.best_score_], color="red", label="best result")
plt.grid()
plt.legend()
plt.ylim((-40, -27))
plt.show()
```


```python

```

### Загрузка данных

# Дерево решений для определения цены фьючерса на индекс RTS по точке минимальных выплат, построенной по опционам.

Загрузка файла с БД по опционам. Формат файла SQLite.


```python
# !curl -o rts_nn.db -L 'https://drive.google.com/uc?export=download&confirm=yes&id=1ML9AKyQexvic_ut2aMstI_7DQ2b6vHrv'  # Закачка файла БД
```

Загрузка данных из БД в DF.


```python
import pandas as pd
import numpy as np
import sqlite3
import matplotlib.pyplot as plt


# connection = sqlite3.connect('rts_nn.db', check_same_thread=True)  # Создание соединения с БД для Colab
connection = sqlite3.connect(r'c:\Users\Alkor\gd\data_quote_db\RTS_nn.db', check_same_thread=True)  # Создание соединения с БД для Local
with connection:
  df_all = pd.read_sql('SELECT * FROM `All_opt`', connection)  # Загрузка данных из БД

print(df_all.to_string(max_rows=6, max_cols=25))  # Проверка того, что загрузилось
```

           TRADEDATE    -22500    -20000    -17500    -15000    -12500    -10000     -7500     -5000     -2500    0      2500      5000      7500     10000     12500     15000     17500     20000  22500  zero_strike   close     low    high
    0     2015-01-05  0.728363  0.605605  0.600028  0.456538  0.454987  0.258267  0.345876  0.183768  0.262591  0.0  0.284517  0.170603  0.359587  0.260492  0.504470  0.458944  0.909248  0.768367    1.0        72500  2100.0   -30.0  6480.0
    1     2015-01-06  0.676081  0.560392  0.555003  0.416899  0.415333  0.229272  0.311330  0.161136  0.233857  0.0  0.289971  0.189553  0.379894  0.285059  0.530621  0.484260  0.916784  0.780905    1.0        72500   980.0 -1300.0  2110.0
    2     2015-01-08  0.548794  0.450866  0.446081  0.330494  0.329217  0.181043  0.258321  0.138457  0.202241  0.0  0.265168  0.160529  0.330973  0.258061  0.506760  0.469379  0.870850  0.757173    1.0        72500  7480.0 -1500.0  8880.0
    ...          ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...       ...       ...    ...          ...     ...     ...     ...
    1985  2022-12-28  0.848746  0.828423  0.817585  0.711167  0.663799  0.582576  0.533955  0.304759  0.111452  0.0  0.135529  0.092753  0.345610  0.537144  0.722648  0.773471  0.870407  0.906975    1.0        97500 -1300.0 -1330.0  1870.0
    1986  2022-12-29  0.724340  0.717070  0.678723  0.663194  0.470939  0.457866  0.381184  0.356907  0.148621  0.0  0.009194  0.283218  0.153112  0.413203  0.641912  0.838733  0.861018  0.954080    1.0        95000  1640.0   600.0  3180.0
    1987  2022-12-30  0.733349  0.708816  0.686451  0.508133  0.486602  0.425623  0.390517  0.182369  0.057725  0.0  0.114708  0.096801  0.301505  0.486329  0.680449  0.759743  0.826896  0.877805    1.0        97500   730.0 -1280.0  1760.0
    

Из признаков (характеристик данных) и целевой переменной (close) сформируем датафрейм, в качестве названий колонок возьмем названия признаков.


```python
features = '-22500 -20000 -17500 -15000 -12500 -10000 -7500 -5000 -2500 0 2500 5000 7500 10000 12500 15000 17500 20000 22500'.split()  # Создание списка признаков
print(features)
```

    ['-22500', '-20000', '-17500', '-15000', '-12500', '-10000', '-7500', '-5000', '-2500', '0', '2500', '5000', '7500', '10000', '12500', '15000', '17500', '20000', '22500']
    

### Очистка данных.

Создание DF из колонок с фичами и целевой колонки. Проверка полученного DF на наличие ячеек с NaN.  


```python
df = df_all[features + ['close']]
print(df.isnull().sum())  # Проверка на NaN
print(f'Всего значений NaN: {df.isnull().sum().sum()}')  # Всего NaN
```

    -22500    0
    -20000    0
    -17500    0
    -15000    0
    -12500    0
    -10000    0
    -7500     0
    -5000     0
    -2500     3
    0         0
    2500      3
    5000      3
    7500      3
    10000     0
    12500     0
    15000     0
    17500     0
    20000     0
    22500     0
    close     0
    dtype: int64
    Всего значений NaN: 12
    

Видно, что ячейки с NaN находятся рядом с нормированным к "0" страйком, значит значения NaN можно заменить на ноль.


```python
df = df.fillna(0)  # Замена NaN на "0"
print(f'Всего значений NaN: {df.isnull().sum().sum()}')  # Всего NaN
```

    Всего значений NaN: 0
    

Данные которые анализируются, являются биржевыми, и среди значений присутствуют выбросы. Для качественного обучения, нужно исключить из обучающей выборки данные с выбросами.  
Проверка данных на выбросы графическим способом.


```python
# Box Plot
import seaborn as sns


sns.set(rc={'figure.figsize':(25,3)})
sns.boxplot(x=df['close'])
```




    <AxesSubplot: xlabel='close'>




    
![png](%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9_%D0%B4%D0%BB%D1%8F_%D1%82%D0%BE%D1%87%D0%BA%D0%B8_%D0%BC%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%B2%D1%8B%D0%BF%D0%BB%D0%B0%D1%82_files/%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9_%D0%B4%D0%BB%D1%8F_%D1%82%D0%BE%D1%87%D0%BA%D0%B8_%D0%BC%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%B2%D1%8B%D0%BF%D0%BB%D0%B0%D1%82_14_1.png)
    


Значения ниже -20000 и выше 20000 будем считать выбросами.  
Создание массивов индексов с выбросами.


```python
# Position of the Outlier
print(np.where(df['close']>20000))
print(np.where(df['close']<-20000))
```

    (array([ 757, 1481, 1482, 1483, 1484, 1485, 1600, 1601, 1602, 1603, 1604,
           1605, 1606, 1607, 1608, 1609, 1610, 1701], dtype=int64),)
    (array([1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302,
           1304, 1305, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794,
           1795, 1796, 1797, 1847, 1858, 1859, 1861, 1862, 1867, 1892],
          dtype=int64),)
    

Удаляем из DF строки с выбросами


```python
print("Old Shape: ", df.shape)
df.drop(np.where(df['close']>20000)[0], inplace=True)
df.drop(np.where(df['close']<-20000)[0], inplace=True)
print("New Shape: ", df.shape)
```

    Old Shape:  (1988, 20)
    New Shape:  (1938, 20)
    

### Разделение данных на обучающую и тестовую выборку.


```python
from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(
    df[features],
    df['close'],
    test_size=0.2,
    shuffle=True,
    random_state=3
)

X_train.shape, y_train.shape, X_test.shape, y_test.shape
```




    ((1550, 19), (1550,), (388, 19), (388,))



### Обучение дерева решений

Инициализируем дерево решений для задачи регрессии и обучим на обучающей выборке (`X_train`) и целевой переменной для обучающих объектов (`y_train`).  
Начнем обучение на параметрах по умолчанию:  
- `max_depth=None`
- `min_samples_leaf=1`
- `max_leaf_nodes=None`


```python
from sklearn.tree import DecisionTreeRegressor


tree = DecisionTreeRegressor(random_state=1)
tree.fit(X_train, y_train)
```




<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=1)</pre></div></div></div></div></div>



Теперь визуализируем наше обученное дерево решений.


```python
from sklearn.tree import plot_tree


# plt.figure(figsize=(22, 15))
# plot_tree(tree, feature_names=features, filled=True);
```

Узнаем, насколько дерево решений обучилось хорошо, для этого сделаем предсказания моделью для обучающей выборке и для тестовой, а затем посчитаем метрику качества средне-квадратичную ошибку.


```python
from sklearn.metrics import mean_squared_error


pred_train = tree.predict(X_train)
pred_test = tree.predict(X_test)

mse_train = mean_squared_error(y_train, pred_train)
mse_test = mean_squared_error(y_test, pred_test)

print(f'MSE на обучении {mse_train:.2f}')
print(f'MSE на тесте {mse_test:.2f}')
```

    MSE на обучении 0.00
    MSE на тесте 57267528.61
    

Метрика на обучении получилась очень маленькая, равная нулю, это говорит нам о том, что во все истинные значения наша модель идеально попала.
Все значения целевого признака из обучения полностью совпадают с предсказанными значениями:


```python
pd.DataFrame({
    'true': y_train,
    'pred': pred_train
})
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>true</th>
      <th>pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>692</th>
      <td>1260.0</td>
      <td>1260.0</td>
    </tr>
    <tr>
      <th>1007</th>
      <td>-20.0</td>
      <td>-20.0</td>
    </tr>
    <tr>
      <th>579</th>
      <td>1320.0</td>
      <td>1320.0</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1150.0</td>
      <td>1150.0</td>
    </tr>
    <tr>
      <th>1095</th>
      <td>10.0</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>953</th>
      <td>2090.0</td>
      <td>2090.0</td>
    </tr>
    <tr>
      <th>1697</th>
      <td>11050.0</td>
      <td>11050.0</td>
    </tr>
    <tr>
      <th>1274</th>
      <td>1840.0</td>
      <td>1840.0</td>
    </tr>
    <tr>
      <th>1719</th>
      <td>2130.0</td>
      <td>2130.0</td>
    </tr>
    <tr>
      <th>1948</th>
      <td>5910.0</td>
      <td>5910.0</td>
    </tr>
  </tbody>
</table>
<p>1550 rows × 2 columns</p>
</div>



На тестовой выборке картина далеко не такая идеальная:


```python
pd.DataFrame({
    'true': y_test,
    'pred': pred_test
})
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>true</th>
      <th>pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1747</th>
      <td>4910.0</td>
      <td>-2740.0</td>
    </tr>
    <tr>
      <th>846</th>
      <td>-70.0</td>
      <td>790.0</td>
    </tr>
    <tr>
      <th>1716</th>
      <td>1520.0</td>
      <td>5260.0</td>
    </tr>
    <tr>
      <th>981</th>
      <td>680.0</td>
      <td>570.0</td>
    </tr>
    <tr>
      <th>312</th>
      <td>-2190.0</td>
      <td>-570.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1858</th>
      <td>-22450.0</td>
      <td>-23770.0</td>
    </tr>
    <tr>
      <th>939</th>
      <td>-2470.0</td>
      <td>700.0</td>
    </tr>
    <tr>
      <th>1729</th>
      <td>3240.0</td>
      <td>-15150.0</td>
    </tr>
    <tr>
      <th>817</th>
      <td>-3410.0</td>
      <td>-4850.0</td>
    </tr>
    <tr>
      <th>361</th>
      <td>-3280.0</td>
      <td>2100.0</td>
    </tr>
  </tbody>
</table>
<p>388 rows × 2 columns</p>
</div>



На тесте отклонения истинных значений от предсказанных больше - это и отображается в метрике MSE на тестовых данных.

Снизим переобучение изменением критериев останова.  
- `max_depth = 10` - глубина дерева. Поставим ему значение меньше, чем на графике дерева. Тем самым запретим дереву быть слишком сложным.  
- `min_samples_leaf = 5` - минимальное количество объектов в одном листе. Изменим аргумент на значение 5, чтобы дерево пыталось строить более обобщенную модель.  
- `max_leaf_nodes = 31` - максимальное количество листьев. Чем больше листьев, тем больше переобучение, потому что модель более сложная. Ограничим макс количество листьев.


```python
# tree = DecisionTreeRegressor(random_state=1, max_depth=10)
# tree.fit(X_train, y_train)

# plt.figure(figsize=(22, 10))
# plot_tree(tree, feature_names=features, filled=True);
```


```python
# tree = DecisionTreeRegressor(random_state=1, min_samples_leaf=5)
# tree.fit(X_train, y_train)

# plt.figure(figsize=(20, 10))
# plot_tree(tree, feature_names=features, filled=True);
```


```python
tree = DecisionTreeRegressor(random_state=1,
                             max_depth=10,
                             min_samples_leaf=5,
                             max_leaf_nodes=31)
tree.fit(X_train, y_train)

pred_train = tree.predict(X_train)
pred_test = tree.predict(X_test)

mse_train = mean_squared_error(y_train, pred_train)
mse_test = mean_squared_error(y_test, pred_test)

print(f'MSE на обучении {mse_train:.2f}')
print(f'MSE на тесте {mse_test:.2f}')
```

    MSE на обучении 11387627.70
    MSE на тесте 63492730.09
    


```python
pred_df = pd.DataFrame({'true': y_test.astype(int),  # Реальные данные
                        'pred': pred_test.astype(int)})  # Предсказанные данные
print(pred_df.to_string(max_rows=100, max_cols=2))
```

           true   pred
    1747   4910   5248
    846     -70    592
    1716   1520   4768
    981     680    592
    312   -2190  -1753
    85     3950   1034
    204   -3910  -2557
    154   -1680  -1317
    746   13850  16499
    487   17080   1545
    1617   1860   -175
    522    4570   1017
    269    2480   1545
    751    9110  -1980
    103   -5280   1034
    456    1600   1017
    378    2840   1017
    1082   -870   1034
    1805  -2110  -1317
    623     280  -2557
    1882   1930   4768
    1403   4290   1545
    3      5150   -989
    112    1140   1034
    83     3170   3512
    1873    -20   1017
    602    -670    592
    1444    680   -175
    1587  11190   -175
    882    -420   -175
    920    2140   -175
    1825   8800  -1317
    1454  -5880   -175
    1092    290   -175
    1197   -110   -175
    1527  -1310  -2557
    1140  -1260   -175
    1892 -31150 -23938
    349    -250   -175
    1204   3950   2222
    1742    160   -175
    843   -1250   1545
    31    19950  10616
    1215    720   1545
    1816    840 -14346
    617    1000    592
    500    3290   1017
    1864   2000   -175
    1064    330   -175
    288    4040   1545
    ...     ...    ...
    1311  10250   5248
    1941   6250   -175
    1161    840   1034
    37     2320   4768
    289    6070   1545
    126   -5580   -175
    1509   -290   2896
    80     -350   -175
    1489  -2570   -175
    1409  -1010  -2557
    1797 -91070 -86221
    805    2440   5248
    824   -4280  -1753
    216   -1050   1034
    554    1970   1017
    1341  14080   5248
    926      10   -175
    706    3200   1545
    1113   3190   5248
    715    3250   1545
    847    -880  -1753
    1826   5650  -1317
    1968   -750   -175
    1753    290   1545
    1235   4490   1017
    1451  -4670   -175
    87     2540   1034
    801    2600   5248
    1229    160   1017
    744   12560  16499
    250   -2730   1545
    1117  -2220   -175
    1012    430   1545
    825     160  -1317
    1530   -760   1017
    148    2400   1034
    499    2990   1017
    1128   -370   1034
    660    5590   1545
    318    3690  -2557
    614    2310    592
    1951   4150   -175
    1982  -2430   -175
    1752   4220   5248
    1141  -2830  -2557
    1858 -22450 -23938
    939   -2470   -175
    1729   3240   1545
    817   -3410  -1753
    361   -3280   1017
    


```python
# df_1 = df.loc[[1000, [features]]]
# df_1 = df.loc[[1000]]  # работает
# df_1 = df.loc[1000, df[features]]
# df_1 = df.loc[1000]

# df_1 = df.loc[[1000]][features]  # работает
print(tree.predict(df.loc[[1000]][features]))
print(tree.predict(df.loc[[1001]][features]))
# df_1
```

    [1545.5]
    [-175.38659794]
    
